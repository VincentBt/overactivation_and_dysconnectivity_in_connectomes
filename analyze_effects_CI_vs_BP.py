import os
import dill
import seaborn as sns
import itertools
from utils_graph_measures import *
from utils_CI_BP import *
from utils_graph_rendering import *
from utils_graph_measures import all_fun_measures
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.cross_decomposition import PLSRegression
from pprint import pprint
import pandas as pd
from compute_effects_CI_vs_BP import *
from sklearn.decomposition import PCA, FactorAnalysis, FastICA
from sklearn.preprocessing import scale
from sklearn.model_selection import cross_val_score
from sklearn.cross_decomposition import PLSRegression
import sys


def plt_scatter(x, y, xlabel='', ylabel=''):
    plt.scatter(x, y)
    if xlabel != '':
        plt.xlabel(xlabel)
    if ylabel != '':
        plt.ylabel(ylabel)
    xx = np.linspace(np.min([x, y]), np.max([x, y]),2)
    plt.plot(xx, xx, linestyle='--')
#     plt.show()


from utils_graph_measures import all_fun_measures
list_measure_names = list(all_fun_measures.keys())



def load_simulated_data(path_look_files, which_ext_input_type, type_graph_loaded, verbose=False, begin=0, k=0.05, 
                        remove_files_bistability=False, 
                        remove_cerebellum_and_vermis=False, remove_ofc=False):
    """
    Load the data generated by compute_effects_CI_vs_BP.py
    !!! Check that there is no problem in the case where k is a list (list_k):
    """
    
    assert which_ext_input_type in ['gaussian_process', 'poisson_process', 'gaussian_process_by_periods', 'gaussian_process_by_periods_not_all_nodes']

    if isinstance(k, float) or isinstance(k, int):
        list_k = [k]
    else:
        list_k = k
    
    suffix_folder = get_suffix_to_folder(type_graph_loaded, remove_cerebellum_and_vermis, remove_ofc)
#     print("path_look_files = {}".format(path_look_files))
    if type_graph_loaded + suffix_folder not in os.listdir(path_look_files):
        print("Data cannot be loaded as they weren't first generated --> run compute_effects_CI_vs_BP.py with the following command: 'python compute_effects_CI_vs_BP.py {}'".format(type_graph_loaded))
#         sys.exit()
#     print("looking at path {}".format(path_look_files + type_graph_loaded + suffix_folder + '/'))
    list_files = os.listdir(path_look_files + type_graph_loaded + suffix_folder + '/')
#     print("{} files to load".format(len(list_files)))
    list_files = [el for el in list_files if '.pkl' in el]
    list_files = [el for el in list_files if which_ext_input_type in el] #select only files with the same generation process for M_ext

    df_all_alpha_all_files = []

    cpt_removing = 0
    
    print("Starting to load ({} files)...".format(len(list_files)))
    for ifile, file in enumerate(list_files):
        if verbose:
            print("ifile = {} (out of {})".format(ifile + 1, len(list_files)))
        #Percentage bar
        sys.stdout.write('\r')
        j = (ifile + 1) / len(list_files)
        sys.stdout.write("[%-20s] %d%%" % ('='*int(20*j), 100*j))
        sys.stdout.flush()
            
        #Load data from simulations
        with open(path_look_files + type_graph_loaded + suffix_folder + '/' + file, 'rb') as f:
            res = dill.load(f)
        M_ext = res.M_ext
        B_history_CI_all = res.B_history_CI_all
        graph = res.graph_G
        list_alpha_c_alpha_d = list(B_history_CI_all.keys())
        
        ############ Check frustration #####################################
        frustration = res.check_frustration_CI(begin=begin)
#         print("frustration", frustration)
        if frustration:
    #         print("frustation")
            continue

        #Skip (or not) this graph
        skipping = res.skipping
        if skipping == True:
            continue
        
        assert (1,1) in list_alpha_c_alpha_d #BP data

        #compute graph measures
        from utils_graph_measures import all_fun_measures
        all_measures_graph = {}
        for measure_name, fun_measure in all_fun_measures.items():
            all_measures_graph[measure_name] = fun_measure(res.graph_G) #before, G was used here
            assert list(all_measures_graph[measure_name].keys()) == list(res.graph_G.nodes()), "{} vs {} ({}): {} and {}".format(len(list(all_measures_graph[measure_name].keys())), len(list(res.graph_G.nodes())), measure_name, list(all_measures_graph[measure_name].keys()), list(res.graph_G.nodes())) 
            all_measures_graph[measure_name] = list(all_measures_graph[measure_name].values())
        # print(all_measures_graph)

        list_measure_names = list(all_measures_graph.keys())

        #plot graph measures
#         for measure_name, measure_val in all_measures_graph.items():
#             plt.plot(measure_val)
#             plt.title(measure_name)
#             plt.show()
        
        begin = 0 #20 #It's fine to use begin = 0 as M_ext starts at 0

        #Load BP data (alpha_c = alpha_d = 1)
        B_history_BP = res.B_history_CI_all[1, 1]
    #     B_history_BP = {key:np.array(val) for key,val in B_history_BP.items()} #already done before saving
        updates_BP = {key: val[1:] - val[:-1] for key,val in B_history_BP.items()}
        squared_updates_BP = {key: val**2 for key,val in updates_BP.items()}     
        sum_squared_updates_BP = sum_squared_updates(updates_BP, begin=begin)
        sum_squared_updates_BP = np.array([sum_squared_updates_BP[node] for node in res.graph_G.nodes])
        sum_abs_updates_BP = sum_abs_updates(updates_BP, begin=begin)
        sum_abs_updates_BP = np.array([sum_abs_updates_BP[node] for node in res.graph_G.nodes])
#         activations_history_BP = get_activations(B_history_BP, method='square_updates_B') # = {key: val**2 for key, val in updates_BP.items()}

        df_all_alpha_file = []

        add_df = True

        for i_alpha, (alpha_c, alpha_d) in enumerate(list_alpha_c_alpha_d):

#             print("(alpha_c; alpha_d) = ({}; {})".format(alpha_c, alpha_d))
            
            #Load CI data
            B_history_CI = res.B_history_CI_all[alpha_c, alpha_d]
    #         B_history_CI = {key:np.array(val) for key,val in B_history_CI.items()} #already done before saving
            updates_CI = {key:val[1:]-val[:-1] for key,val in B_history_CI.items()}
            squared_updates_CI = {key:val**2 for key,val in updates_CI.items()}        
            sum_squared_updates_CI = sum_squared_updates(updates_CI, begin=begin)
            sum_squared_updates_CI = np.array([sum_squared_updates_CI[node] for node in res.graph_G.nodes])
            sum_abs_updates_CI = sum_abs_updates(updates_CI, begin=begin)
            sum_abs_updates_CI = np.array([sum_abs_updates_CI[node] for node in res.graph_G.nodes])
#             activations_history_CI = get_activations(B_history_CI, method='square_updates_B') # = {key: val**2 for key, val in updates_CI.items()}
    
            x = np.array(list(all_measures_graph.values())).T #TODO (maybe): center and normalize the values by measure_name?
            dict_data = all_measures_graph.copy()
            dict_data['alpha_c'] = alpha_c
            dict_data['alpha_d'] = alpha_d
            
            if remove_files_bistability:
                if detect_bistability_dict(B_history_CI, verbose=False):
#                     print("Bistability detected (alpha_c = {}, alpha_d = {})".format(alpha_c, alpha_d))
                    add_df = False
                    break
            
            for k in list_k:
                
                activations_history_BP = get_activations(B_history_BP, method='leaky_belief', k=k)
                total_activation_BP = get_total_activation(activations_history_BP, begin=begin)
                total_activation_BP = np.array([total_activation_BP[node] for node in res.graph_G.nodes])  
                squared_updates_BP_flatten = np.array([val[begin:] for key,val in squared_updates_BP.items()]).flatten()
#                 average_B_BP = {key: np.mean(np.abs(val)) for key,val in B_history_BP.items()}
#                 average_B_BP = np.array([average_B_BP[node] for node in res.graph_G.nodes])
                
                activations_history_CI = get_activations(B_history_CI, method='leaky_belief', k=k)
                total_activation_CI = get_total_activation(activations_history_CI, begin=begin)
                total_activation_CI = np.array([total_activation_CI[node] for node in res.graph_G.nodes])
                squared_updates_CI_flatten = np.array([val[begin:] for key,val in squared_updates_CI.items()]).flatten()
#                 average_B_CI = {key: np.mean(np.abs(val)) for key,val in B_history_CI.items()}
#                 average_B_CI = np.array([average_B_CI[node] for node in res.graph_G.nodes])
                
#                 print(alpha_c, alpha_d, np.max(squared_updates_CI_flatten))
                #Plotting cases with intermediate np.max(squared_updates_CI_flatten), to dertermine the threshold
#                 if (np.max(squared_updates_CI_flatten) > 2) and (np.max(squared_updates_CI_flatten) < 10):
#                     plt.figure(figsize=(18,6))
#                     for key,val in B_history_CI.items():
#                         plt.plot(val, label=key)
#                     plt.show()
                #Filtering files for which there is frustration
                if np.max(squared_updates_CI_flatten) > 3: #if np.max(squared_updates_CI_flatten - squared_updates_BP_flatten) > 1: ##filter for gaussian_process as M_ext
                    add_df = False
#                     print("Frustration detected (alpha_c = {}, alpha_d = {})".format(alpha_c, alpha_d))
#                     plt.title("alpha_c = {}, alpha_d = {}".format(alpha_c, alpha_d))
#                     plt_scatter(squared_updates_BP_flatten, squared_updates_CI_flatten, 'squared_updates_BP', 'squared_updates_CI')
#                     plt.show()
#                     plt.figure(figsize=(14,6))
#                     plt.subplot(1,2,1)
#                     plt.title("alpha_c = {}, alpha_d = {}".format(alpha_c, alpha_d))
#                     plt.scatter(sum_squared_updates_BP, sum_squared_updates_CI)
#                     plt.subplot(1,2,2)
#                     plt.title("alpha_c = {}, alpha_d = {}".format(alpha_c, alpha_d))
#                     plt.scatter(sum_squared_updates_BP, sum_squared_updates_CI)
#         #             plt.ylim(np.min(sum_squared_updates_BP), np.max(sum_squared_updates_BP))
#                     plt.ylim(np.min(sum_squared_updates_BP), 2*np.max(sum_squared_updates_BP))
#                     xx = np.linspace(np.min(sum_squared_updates_BP), np.max(sum_squared_updates_BP), 2)
#                     plt.plot(xx, xx, linestyle='--', color='black')
#                     plt.show()
#                     plt.figure(figsize=(18,6))
#                     for key,val in B_history_CI.items():
#                         plt.plot(val, label=key)
#                     plt.show()
                    break
                 
                d_total_activation_BP = get_total_activation(activations_history_BP, begin=begin)
                d_total_activation_CI = get_total_activation(activations_history_CI, begin=begin)
                d_overactivation = {key: 100 * (d_total_activation_CI[key] - d_total_activation_BP[key]) / 
                                    d_total_activation_BP[key]
                                    for key in d_total_activation_CI.keys()} #overactivation in %
                threshold_overactivation = 60
                n_nodes_overly_activated = np.sum(np.array(list(d_overactivation.values())) > threshold_overactivation)
    #             print("There are {} nodes with overly big overactivation".format(n_nodes_overly_activated))
                if n_nodes_overly_activated > 1:
                    print("detected low frustration (ifile = {} out of {})".format(ifile + 1, len(list_files)))
                    add_df = False
                    break
        
        
                #Choose the regressed variable
                #y = sum_squared_updates_CI - sum_squared_updates_BP
                y = total_activation_CI - total_activation_BP
                y2 = 100 * (total_activation_CI - total_activation_BP) / total_activation_BP #TODO: transform into dictionnaries! (the order of the nodes shouldn't be important)
#                 y = average_B_CI - average_B_BP
#                 dict_data['sum_sq_upd_CI_min_BP (k='+str(k)+')'] = y
                dict_data['diff_activation_CI_BP (k='+str(k)+')'] = y
                dict_data['percent_diff_activation_CI_BP (k='+str(k)+')'] = y2

#                 if np.max(y) == np.nan:
#                     print("mistake (k={}, ifile={})".format(k, ifile+1))
#                 print("shape for y = ", total_activation_CI.shape, total_activation_BP.shape)
#                 print("x.shape = {}, y.shape = {}".format(x.shape, y.shape))
                
            if add_df == False:
                break
            df = pd.DataFrame.from_dict(dict_data)
            df_all_alpha_file.append(df)

        if add_df:
            df_all_alpha_file = pd.concat(df_all_alpha_file)
#             print("added file {}".format(ifile + 1))
            df_all_alpha_all_files.append(df_all_alpha_file)
        else:
            cpt_removing += 1
#             print("not taking file {}".format(ifile + 1))
            
    if len(df_all_alpha_all_files) == 0:
        print("All networks were frustrated or led to bistability")
    df_all_alpha_all_files = pd.concat(df_all_alpha_all_files)
    if remove_files_bistability == False:
        print("Data loaded (removed {} files out of {} because networks were frustrated)".format(cpt_removing, len(list_files)))
    else:
        print("Data loaded (removed {} files out of {} because networks were frustrated or led to bistability)".format(cpt_removing, len(list_files)))
#     display(df_all_alpha_all_files)
    return df_all_alpha_all_files






def do_reg(df_select, list_regressors, regressed_variable='diff_activation_CI_BP', check_reg=True, return_coef=False,return_reg=False, verbose=False, method='Linear_Regression'):
    print("Regression method: {}".format(method))
    
    x = df_select[list_regressors].values
#     print("x.shape", x.shape)
#     x = (scale(x)) #normalization (same as doing x = (x - np.mean(x, axis=0)) / np.std(x, axis=0))
    y = df_select[[regressed_variable]].values.flatten()
#     print("x.shape = {}, y.shape = {}".format(x.shape, y.shape))

    #shuffle the data
    shuffling_indices = np.arange(x.shape[0])
    np.random.shuffle(shuffling_indices)
#     print("5 first elements of shuffling_indices:", shuffling_indices[:5])
    x = x[shuffling_indices,:]
    y = y[shuffling_indices]
    print("x.shape = {}, y.shape = {}".format(x.shape, y.shape))
    
    if method == "Linear_Regression":
        reg = LinearRegression(fit_intercept=True).fit(x, y) #with constant
#         reg = LinearRegression(fit_intercept=False).fit(x, y) #without constant
        reg_coef = reg.coef_
    elif "PLS_Regression" in method: #Partial Least Squares Regression
        #Using cross-validation to select the right number of components for PLSRegression (see https://scikit-learn.org/stable/modules/cross_validation.html)
        for n_components in range(1, x.shape[1] + 1):
            clf = PLSRegression(n_components=n_components)
            n_cv = 10 #5  #number of splits for the Kfold
            scores = cross_val_score(clf, x, y, cv=n_cv, scoring='neg_mean_squared_error')
            print("n_components = {}, MSE = {}".format(n_components, - scores.mean()))
#             print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))
        
        name_method = "PLS_Regression"
        n_components = 6 if method == name_method else int(method[len(name_method):])
        n_components = np.min([n_components, len(list_regressors)])
#         print("n_components (PLS Regression) =", n_components)
        reg = PLSRegression(n_components=n_components).fit(x, y) #with constant (seems fine even with constant)
#         reg = PLSRegression(n_components=n_components).fit(x, y) #without constant
        reg_coef = reg.coef_.flatten()
    elif "Factor_Analysis" in method:
        print("not implemented")
        sys.exit()
        #The inverse_transformation method does not exist in scikit-learn, and I didnt' understand how to compute it by hand (and if it's possible)
        factor_an = FactorAnalysis(n_components=5)
        factor_an.fit(x)
        x_new = factor_an.transform(x)
        print(x.shape, x_new.shape)
        print(factor_an.components_.shape)
        print(x.dot(factor_an.components_.T).shape)
        print(x.dot(factor_an.components_.T))
        print(x_new)
        reg = LinearRegression(fit_intercept=True).fit(x_new, y) #with constant
#         reg = LinearRegression(fit_intercept=True).fit(x, y) #without constant
        print(factor_an.components_.shape)
        for component in factor_an.components_:
            plt.plot(component)
        plt.show()
        reg_coef = factor_an.inverse_transform(reg.coef_) #does not exist
    elif "ICA" in method: #Partial Least Squares Regression
        name_method = "ICA"
        n_components = 5 if method == name_method else int(method[len(name_method):])
#         print("n_components =", n_components)
        ica = FastICA(n_components=n_components)
        x_new = ica.fit_transform(x)
#         print("shapes before and after", x.shape, x_new.shape)
        reg = LinearRegression(fit_intercept=True).fit(x_new, y) #with constant
#         reg = LinearRegression(fit_intercept=True).fit(x, y) #without constant
        reg_coef = ica.inverse_transform((reg.coef_).reshape(1,-1)).flatten()
    elif "Principal_Component_Regression" in method: #I followed the definition of PCR to code it "by hand" - but I'm not sure how to compute the scores (for cross-validation, which is useful to determine the number of components) because I would also need to define it by-hand, contrary to PLS_Regression.
        name_method = "Principal_Component_Regression"
        print(method[len(name_method):])
        n_components = 5 if method == name_method else int(method[len(name_method):])
#         print("n_components =", n_components)
        pca = PCA(n_components=n_components) #the number of components is arbitrary here. TODO = find a good value automatically.
        pca.fit(x)
    #     print("pca.explained_variance_ratio_", pca.explained_variance_ratio_)
    #     print("cumulative variance", np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)) #Variance (% cumulative) explained by the principal components
    #     print("singular values", pca.singular_values_)
    #     pca_components = pca.components_ #principal components
    #     for i, pca_component in enumerate(pca_components):
    #         plt.plot(pca_component, label="PCA component #{}".format(i))
    #         plt.show()
        # plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        # plt.show()
        #Do linear regression on the transformed data
        x_new = pca.transform(x)
#         print("x_new.shape", x_new.shape)
        reg = LinearRegression(fit_intercept=True).fit(x_new, y) #with constant
#         reg = LinearRegression(fit_intercept=False).fit(x_new, y) #without constant
#         print((reg.coef_).shape)
        reg_coef = pca.inverse_transform(reg.coef_)
#         print("reg_coef.shape", reg_coef.shape)

#     r_squared = reg.score(x, y)
#     print("r_squared", r_squared)
#     vif = 1/(1 - r_squared) if r_squared != 1 else np.inf
#     print("vif", vif)
    
#     vifs = pd.Series(np.linalg.inv(df_select[list_regressors].corr().values).diagonal(), index=df_select[list_regressors].corr().index) 
#     print("vifs", vifs)
    
    # print(reg.coef_) #does not include the constant (in the case fit_intercept=True)
#     print((reg.coef_).shape)
    # pprint(dict(zip(list(list_regressors), reg.coef_)))
    # print(dict(zip(list(list_regressors), reg.coef_)))
    if verbose:
        for key,val in dict(zip(list(list_regressors), reg_coef)).items():
            print("{}: {}".format(key, val))
    if check_reg:
        if method in ["Principal_Component_Regression", "ICA", "Factor_Analysis"]:
            check_regression(reg, x_new, y)
        else:
            check_regression(reg, x, y)
    print("return_coef = {}, return_reg = {}".format(return_coef, return_reg))
    if return_coef and return_reg:
        return reg_coef, reg
    elif return_coef:
        return reg_coef
    


def check_regression(reg, x, y):
    """
    Check how good the linear model is by plotting y vs y_pred
    """
    y_pred = reg.predict(x)  # make predictions
    def plt_scatter(x, y, xlabel='', ylabel=''):
        plt.scatter(x, y)
        if xlabel != '':
            plt.xlabel(xlabel)
        if ylabel != '':
            plt.ylabel(ylabel)
    #     xx = np.linspace(np.min([x, y]), np.max([x, y]),2)
        xx = np.linspace(np.min([y]), np.max([y]),2)
        plt.plot(xx, xx, linestyle='--',color='black')
    #     plt.show()
    # plt.scatter(y, y_pred)
    plt_scatter(y, y_pred, 'true y', 'predicted y')
    plt.show()